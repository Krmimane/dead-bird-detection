{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df250892",
   "metadata": {},
   "source": [
    "# Pipeline de prÃ©paration des donnÃ©es pour la dÃ©tection dâ€™oiseaux morts\n",
    "\n",
    "Ce notebook dÃ©crit lâ€™ensemble du workflow de prÃ©paration des donnÃ©es utilisÃ© pour lâ€™entraÃ®nement du modÃ¨le de dÃ©tection dâ€™oiseaux morts Ã  partir dâ€™images aÃ©riennes de haute rÃ©solution.\n",
    "\n",
    "En raison de la grande taille des images originales et de la trÃ¨s petite taille relative des oiseaux, un apprentissage direct sur les images brutes est peu efficace. Un pipeline de prÃ©-traitement en plusieurs Ã©tapes est donc mis en place, comprenant :\n",
    "\n",
    "- lâ€™analyse statistique du jeu de donnÃ©es initial,\n",
    "- le dÃ©coupage des images en tuiles de 512Ã—512 pixels,\n",
    "- lâ€™augmentation de donnÃ©es,\n",
    "- la fusion des jeux de donnÃ©es gÃ©nÃ©rÃ©s,\n",
    "- lâ€™analyse finale du dataset utilisÃ© pour lâ€™apprentissage.\n",
    "\n",
    "Lâ€™objectif est dâ€™amÃ©liorer la visibilitÃ© des objets, dâ€™augmenter la diversitÃ© des Ã©chantillons et de renforcer la capacitÃ© de gÃ©nÃ©ralisation du modÃ¨le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b77bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b3f85",
   "metadata": {},
   "source": [
    "## 1. Analyse du jeu de donnÃ©es initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27589c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(root):\n",
    "    stats = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(root, split, \"images\")\n",
    "        lbl_dir = os.path.join(root, split, \"labels\")\n",
    "\n",
    "        if not os.path.exists(img_dir):\n",
    "            continue\n",
    "\n",
    "        images = glob.glob(os.path.join(img_dir, \"*.jpg\")) + glob.glob(os.path.join(img_dir, \"*.png\"))\n",
    "        labels = glob.glob(os.path.join(lbl_dir, \"*.txt\"))\n",
    "\n",
    "        n_boxes = 0\n",
    "        n_empty = 0\n",
    "        for f in labels:\n",
    "            with open(f) as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) == 0:\n",
    "                    n_empty += 1\n",
    "                n_boxes += len(lines)\n",
    "\n",
    "        stats.append([split, len(images), len(labels), n_boxes, n_empty])\n",
    "\n",
    "    return pd.DataFrame(stats, columns=[\"Split\", \"Images\", \"Labels\", \"Boxes\", \"Empty\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e17a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = \"augmented_dataset\"\n",
    "\n",
    "def count_labels(split_path):\n",
    "    label_dir = os.path.join(split_path, \"labels\")\n",
    "    img_dir = os.path.join(split_path, \"images\")\n",
    "\n",
    "    total_imgs = 0\n",
    "    empty_labels = 0\n",
    "    non_empty_labels = 0\n",
    "\n",
    "    for file in os.listdir(label_dir):\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        total_imgs += 1\n",
    "        path = os.path.join(label_dir, file)\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "\n",
    "        if content == \"\":\n",
    "            empty_labels += 1\n",
    "        else:\n",
    "            non_empty_labels += 1\n",
    "\n",
    "    print(f\"\\nðŸ“‚ {os.path.basename(split_path)}\")\n",
    "    print(f\"Images        : {total_imgs}\")\n",
    "    print(f\"Labels vides  : {empty_labels}\")\n",
    "    print(f\"Labels pleins : {non_empty_labels}\")\n",
    "\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    split_path = os.path.join(DATASET_PATH, split)\n",
    "    if os.path.exists(split_path):\n",
    "        count_labels(split_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATASET = \"exports\"\n",
    "\n",
    "df_original = analyze_dataset(ORIGINAL_DATASET)\n",
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a051d",
   "metadata": {},
   "source": [
    "## 2. DÃ©coupage des images (Tiling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lancement du script de dÃ©coupage...\")\n",
    "os.system(\"python test_split.py\")\n",
    "print(\"DÃ©coupage terminÃ©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILED_DATASET = \"PIC_2_test_tiled_512_v2\"\n",
    "df_tiled = analyze_dataset(TILED_DATASET)\n",
    "df_tiled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d351f",
   "metadata": {},
   "source": [
    "## 3. Augmentation de donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lancement du script dâ€™augmentation...\")\n",
    "os.system(\"python data_augmentation.py\")\n",
    "print(\"Augmentation terminÃ©e.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_DATASET = \"C:\\\\Users\\\\DELL\\\\Documents\\\\S9\\\\Projet_Pic\\\\dead-bird-detection\\\\augmented_dataset\"\n",
    "\n",
    "df_aug = analyze_dataset(AUG_DATASET)\n",
    "df_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f09db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_DATASET = \"augmented_dataset\"\n",
    "df_aug = analyze_dataset(AUG_DATASET)\n",
    "df_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac140e9",
   "metadata": {},
   "source": [
    "## 4. Fusion des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ceda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_ROOT = \"final_dataset\"\n",
    "\n",
    "def merge_datasets(src_roots, dst_root):\n",
    "    for split in [\"train\",\"test\",\"valid\"]:\n",
    "        for root in src_roots:\n",
    "            img_dir = os.path.join(root, split, \"images\")\n",
    "            lbl_dir = os.path.join(root, split, \"labels\")\n",
    "\n",
    "            out_img = os.path.join(dst_root, split, \"images\")\n",
    "            out_lbl = os.path.join(dst_root, split, \"labels\")\n",
    "\n",
    "            os.makedirs(out_img, exist_ok=True)\n",
    "            os.makedirs(out_lbl, exist_ok=True)\n",
    "\n",
    "            for f in glob.glob(os.path.join(img_dir, \"*\")):\n",
    "                os.system(f'copy \"{f}\" \"{out_img}\"')\n",
    "\n",
    "            for f in glob.glob(os.path.join(lbl_dir, \"*\")):\n",
    "                os.system(f'copy \"{f}\" \"{out_lbl}\"')\n",
    "\n",
    "merge_datasets([TILED_DATASET, AUG_DATASET], COMBINED_ROOT)\n",
    "print(\"Fusion terminÃ©e.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = analyze_dataset(COMBINED_ROOT)\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Chemin vers le dataset\n",
    "dataset_path = Path(\"final_dataset\")\n",
    "\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    images_path = dataset_path / split / \"images\"\n",
    "    labels_path = dataset_path / split / \"labels\"\n",
    "    \n",
    "    image_files = list(images_path.glob(\"*.jpg\")) + list(images_path.glob(\"*.png\"))\n",
    "    label_files = list(labels_path.glob(\"*.txt\"))\n",
    "    \n",
    "    print(f\"\\n=== Split: {split} ===\")\n",
    "    print(f\"Nombre d'images: {len(image_files)}\")\n",
    "    \n",
    "    # VÃ©rifier la dimension des images\n",
    "    wrong_dim = []\n",
    "    for img_file in image_files:\n",
    "        with Image.open(img_file) as img:\n",
    "            if img.size != (512, 512):\n",
    "                wrong_dim.append(img_file.name)\n",
    "    if wrong_dim:\n",
    "        print(f\"Images avec mauvaise dimension (pas 512x512): {len(wrong_dim)}\")\n",
    "        print(wrong_dim[:5], \"...\" if len(wrong_dim) > 5 else \"\")\n",
    "    else:\n",
    "        print(\"Toutes les images sont de 512x512 âœ…\")\n",
    "    \n",
    "    # Analyse des labels\n",
    "    total_annotations = 0\n",
    "    empty_labels = 0\n",
    "    out_of_bounds = 0\n",
    "    \n",
    "    for lbl_file in label_files:\n",
    "        with open(lbl_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 0:\n",
    "                empty_labels += 1\n",
    "            for line in lines:\n",
    "                total_annotations += 1\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"Format incorrect: {lbl_file.name}\")\n",
    "                    continue\n",
    "                _, x, y, w, h = map(float, parts)\n",
    "                if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                    out_of_bounds += 1\n",
    "    \n",
    "    print(f\"Nombre total d'annotations: {total_annotations}\")\n",
    "    print(f\"Images sans annotations: {empty_labels}\")\n",
    "    print(f\"Annotations hors limites [0,1]: {out_of_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ff788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"final_dataset\")\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "deleted_images = 0\n",
    "\n",
    "for split in splits:\n",
    "    images_dir = dataset_path / split / \"images\"\n",
    "    labels_dir = dataset_path / split / \"labels\"\n",
    "\n",
    "    print(f\"\\n=== Nettoyage du split {split} ===\")\n",
    "\n",
    "    for label_file in labels_dir.glob(\"*.txt\"):\n",
    "        remove_sample = False\n",
    "\n",
    "        with open(label_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    remove_sample = True\n",
    "                    break\n",
    "\n",
    "                _, x, y, w, h = map(float, parts)\n",
    "\n",
    "                if not (0 <= x <= 1 and 0 <= y <= 1 and\n",
    "                        0 <= w <= 1 and 0 <= h <= 1):\n",
    "                    remove_sample = True\n",
    "                    break\n",
    "\n",
    "        if remove_sample:\n",
    "            image_name = label_file.stem\n",
    "            image_path = None\n",
    "\n",
    "            for ext in [\".jpg\", \".png\", \".jpeg\"]:\n",
    "                candidate = images_dir / f\"{image_name}{ext}\"\n",
    "                if candidate.exists():\n",
    "                    image_path = candidate\n",
    "                    break\n",
    "\n",
    "            # Suppression\n",
    "            label_file.unlink()\n",
    "            if image_path:\n",
    "                image_path.unlink()\n",
    "                deleted_images += 1\n",
    "\n",
    "    print(f\"Images supprimÃ©es dans {split}\")\n",
    "\n",
    "print(f\"\\nTotal images supprimÃ©es : {deleted_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "DATASET_PATH = \"final_dataset\"\n",
    "\n",
    "KEEP_RATIOS = {\n",
    "    \"train\": 1,\n",
    "    \"valid\": 1\n",
    "}\n",
    "\n",
    "\n",
    "def clean_split(split):\n",
    "    img_dir = os.path.join(DATASET_PATH, split, \"images\")\n",
    "    lbl_dir = os.path.join(DATASET_PATH, split, \"labels\")\n",
    "\n",
    "    empty_files = []\n",
    "\n",
    "    for lbl_file in os.listdir(lbl_dir):\n",
    "        if not lbl_file.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(lbl_dir, lbl_file)\n",
    "        with open(path) as f:\n",
    "            content = f.read().strip()\n",
    "\n",
    "        if content == \"\":\n",
    "            empty_files.append(lbl_file)\n",
    "\n",
    "    total_empty = len(empty_files)\n",
    "    keep_n = int(total_empty * KEEP_RATIOS[split])\n",
    "\n",
    "    random.shuffle(empty_files)\n",
    "    keep_files = set(empty_files[:keep_n])\n",
    "    remove_files = empty_files[keep_n:]\n",
    "\n",
    "    for lbl_file in remove_files:\n",
    "        img_file = os.path.splitext(lbl_file)[0] + \".jpg\"\n",
    "\n",
    "        lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "\n",
    "        if os.path.exists(lbl_path):\n",
    "            os.remove(lbl_path)\n",
    "        if os.path.exists(img_path):\n",
    "            os.remove(img_path)\n",
    "\n",
    "    print(f\"\\nðŸ“‚ {split}\")\n",
    "    print(f\"Empty labels total : {total_empty}\")\n",
    "    print(f\"Kept              : {keep_n}\")\n",
    "    print(f\"Removed           : {len(remove_files)}\")\n",
    "\n",
    "\n",
    "for split in [\"train\", \"valid\"]:\n",
    "    clean_split(split)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
